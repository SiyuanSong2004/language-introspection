# language-introspection
code and data for paper [Language Models Fail to Introspect About Their Knowledge of Language](https://arxiv.org/abs/2503.07513) by Siyuan Song, Jennifer Hu and Kyle Mahowald.

Please cite our paper if you find our materials useful:
```
@misc{song2025languagemodelsfailintrospect,
      title={Language Models Fail to Introspect About Their Knowledge of Language}, 
      author={Siyuan Song and Jennifer Hu and Kyle Mahowald},
      year={2025},
      eprint={2503.07513},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.07513}, 
}
```

### Dependencies
To replicate our analyses, you only need the data processing `pandas`, and various libraries in `R`. 
Our experiments rely on [`minicons`](https://github.com/kanishkamisra/minicons), a library based on the Huggingface `transformers` for behavioral and representational analyses of language models. Other required packages are `bitsandbytes` (for model quantization) and `tqdm`.

###### Set up environment with required python packages for experiment and analysis
- Create a new environment for this project and activate:
  
   `conda create -n lang-introspection python=3.9`
   `conda activate lang-introspection`

- install required packages with `pip`:
  `pip install -r requirements.txt`

###### Install R libraries
- Run the following in the R Console:
  `packages <- readLines("r_requirements.txt")`
  `install.packages(packages)`



### Testsuites
Here is an overview of the testsuites used in our study:
| Dataset      | Source    | Description | Experiment |
|--------------|--------------|-------------|------------|
| [`BLiMP-LI`](testsuites/BLIMPLI.csv) | [Warstadt et al. (2020)](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00321/96452/BLiMP-The-Benchmark-of-Linguistic-Minimal-Pairs), [Sprouse et al.(2013)](https://www.sciencedirect.com/science/article/pii/S0024384113001526?casa_token=En7lS3L9HD8AAAAA:v3Vl5HIRv9HGmbXx7uMqBiMjs7FdvDJ6lwtTsYOAekvhwnCdr4CsTClZG6HQ2UyyAEXWOTGBHSs), [Mahowald et al. (2016)](https://muse.jhu.edu/pub/24/article/629764/summary?casa_token=TuNaYv4FF-EAAAAA:TE7EgNGyf1pTe9kTgVW1uReSIo5f9k7HS7L4ZAdM31Llxyreh9bS9n_6MNeMLvGKyYazTSH4PAs) | 670 minimal pairs sampled from `BLiMP` and 380 pairs from two papers on human acceptability judgment | 1 |
| [`wikipedia`](testsuites/wikipedia.csv)  | [Wikipedia Vital Articles (Level 3)](https://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Level/3) | 1000 sentences sampled from wikipedia vital articles.| 2 | 
| [`news`](testsuites/news.csv) | -- | 1000 sentences sampled from news articles in late October, 2024. | 2 |
| [`nonsense`](testsuites/jabberwocky.csv)\* | -- | 1000 'nonsense grammatical sentences' generated by replacing words in the sentences in the `news` dataset. Named as `jabberwocky` in this repo. | 2 |
| [`randomseq`](testsuites/jabberwocky.csv) | -- | 1000 randomly sequences with randomly picked words. | 2 |


\* The dataset was originally named as `jabberwocky` and we changed the name to `nonsense` when analyzing results and writing our paper. Therefore, the name of dataset in [`testsuites`](testsuites) and [`model_output`](model_output/continuation) is `jabberwocky`. They refer to the same dataset.

See [rawdata/README.md](rawdata/README.md) for details on dataset construction.



### Experiments
#### scripts in `\src`
[`models.py`](src/models.py): basic class and methods for language models.
[`blimpli.py`](src/models.py): conduct LM grammaticality judgment with both `Direct` (probability measurement) and `Meta` (metalinguistic prompting) methods; save results to `model_output/acceptability`
[`continuation_final.py`](src/models.py):conduct word prediction experiment with both `Direct` and `Meta` methods. save results to `model_output/continuation`.


#### experiment options
* `-i`, `--input`: Path to the CSV file in [`testsuites`](testsuites) containing stimuli.
* `-o`, `--output`: Path to the output directory where results will be saved.
* `--model`: Model name to be used for evaluation.
* `--revision`: Optional model revision or version.
* `--accelerate`: Enable accelerate mode for loading models on multi-gpus.
* `--instruct`: Evaluate instruction-tuned models with specified settings, including the chat template.
* `--quantization`: Enable 4-bit model quantization.

#### example commands
Our experiments were mainly conducted on 40GB Nvidia A40 and 40GB Nvidia A100 GPUs. Run the following commands in the root directory to conduct the experiment:
- experiment 1: `python src/blimpli.py -i "BLIMPLI.csv" -o model_output/acceptability --model Qwen/Qwen2.5-72B-Instruct --instruct --accelerate --quantization`
(run acceptability judgment experiment on Qwen2.5-72B-Instruct with multi gpus and 4-bit quant)
- experiment 2: `python src/continuation_final.py -i "wikipedia.csv" -o model_output/continuation --model "allenai/OLMo-2-1124-13B" --revision 'stage2-ingredient3-step11931-tokens100B' --accelerate` 
(run word prediction experiment on OLMO-13B-Seed3 with multi gpus and wikipedia dataset)

#### Models evaluated in our experiments
We evaluated 21 models in total, including 15 default or `main` models, and 6 OLMo models trained on same data and different seeds. For ease of experiment replication, we provide the Hugging Face IDs for all models used. Models marked with `\*` are loaded with 4-bit quantization due to the computational resource limitations.
###### Llama models
```
meta-llama/Llama-3.1-8B
meta-llama/Llama-3.1-8B-Instruct
meta-llama/Llama-3.1-70B  *
meta-llama/Llama-3.1-70B-Instruct  *
meta-llama/Llama-3.3-70B-Instruct  *
```
###### Qwen models
```
Qwen/Qwen2.5-1.5B
Qwen/Qwen2.5-1.5B-Instruct
Qwen/Qwen2.5-7B
Qwen/Qwen2.5-7B-Instruct
Qwen/Qwen2.5-72B  *
Qwen/Qwen2.5-72B-Instruct  *
```
###### Mistral model
```
mistralai/Mistral-Large-Instruct-2411  *
```
###### OLMo models
```
allenai/OLMo-2-1124-7B
allenai/OLMo-2-1124-13B
```
For 7B models, we used the default version and the following three versions/checkpoints trained on different seeds ( use `-revision` with our experiment script ):
```
stage2-ingredient1-step11931-tokens50B
stage2-ingredient2-step11931-tokens50B
stage2-ingredient3-step11931-tokens50B
```
For 13B models, we used `main` and the following:
```
stage2-ingredient1-step11931-tokens100B
stage2-ingredient2-step11931-tokens100B
stage2-ingredient3-step11931-tokens100B
```
### Analysis

1. Use [`preparedata.ipynb`](preparedata.ipynb) to process the model output data and store the results of all models in [`alldata/expr1.csv`](alldata/expr1.csv) and [`alldata/expr2.csv`](alldata/expr2.csv). In the processed `.csv` files, `SumLP_diff` corresponds to $\Delta$Direct in our paper and `diff` corresponds to $\Delta$Meta.  
2. Use [`analysis.R`](analysis.R) to analyze data, save figures to [`figures`](figures) and save extracted linear models to [`lm`](lm). To simplify the tables in our paper, we only saved `Estimate`, `Std. Error` and `Signif.code` in the `.tex` files. Please modify the final rows in the R script to save other information with `texreg`.

